# Prodigy-Task1
Train a model to generate coherent and contextually relevant text based on a given prompt. Starting with GPT-2, a transformer model developed by OpenAI, this project demonstrates how to fine-tune the model on a custom dataset to create text that mimics the style and structure of your training data.
# Task 1 â€“ Text Generation with GPT-2

**Circle with gradient**  
**Blue Gradient Background**  

**Comillas Negras**  
**Circle with gradient**  
**ProDigy InfoTech**

---

## Project Overview

Train a model to generate coherent and contextually relevant text based on a given prompt. Starting with **GPT-2**, a transformer model developed by OpenAI, this project demonstrates how to fine-tune the model on a custom dataset to create text that mimics the style and structure of your training data.

---

## Features

- Fine-tuning GPT-2 on custom datasets  
- Generating contextually relevant text  
- Maintaining style and structure of training data  
- Easy-to-use training and generation scripts

---

## Getting Started

### Prerequisites

- Python 3.8+
- PyTorch
- Transformers library by Hugging Face

```bash
pip install torch transformers
